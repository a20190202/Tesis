{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35ef68b",
   "metadata": {},
   "source": [
    "# Preprocesamiento II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3471ae0c",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc28c7",
   "metadata": {},
   "source": [
    "### 5.1 Creacion de los subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebfb58b",
   "metadata": {},
   "source": [
    "#### i. Creacion del JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "799f034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e33f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset_data_augmentation(\n",
    "    image_folder_path,\n",
    "    coco_json_path,\n",
    "    output_image_folder_path,\n",
    "    output_json_path,\n",
    "    num_augmentations\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies data augmentation to images based on COCO annotations and saves the augmented images and new annotations.\n",
    "\n",
    "    Parameters:\n",
    "        image_folder_path (str): Path to the original image folder.\n",
    "        coco_json_path (str): Path to the original COCO JSON annotations.\n",
    "        output_image_folder_path (str): Directory to save augmented images.\n",
    "        output_json_path (str): Path to save the new COCO JSON file.\n",
    "        num_augmentations (int): Number of augmentations to apply per image.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_image_folder_path, exist_ok=True)\n",
    "\n",
    "    with open(coco_json_path, 'r', encoding='utf-8') as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    transform = A.Compose([\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.9),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "        A.MotionBlur(blur_limit=3, p=0.3)\n",
    "    ], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "\n",
    "    new_images = []\n",
    "    new_annotations = []\n",
    "    new_image_id = max(img['id'] for img in coco['images']) + 1\n",
    "    new_annotation_id = max(ann['id'] for ann in coco['annotations']) + 1\n",
    "\n",
    "    image_id_to_anns = {}\n",
    "    for ann in coco['annotations']:\n",
    "        image_id_to_anns.setdefault(ann['image_id'], []).append(ann)\n",
    "\n",
    "    for img_info in tqdm(coco['images'], desc=\"Augmenting images\"):\n",
    "        img_path = os.path.join(image_folder_path, img_info['file_name'])\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        anns = image_id_to_anns.get(img_info['id'], [])\n",
    "        bboxes = [ann['bbox'] for ann in anns]\n",
    "        category_ids = [ann['category_id'] for ann in anns]\n",
    "\n",
    "        if not bboxes:\n",
    "            continue\n",
    "\n",
    "        for i in range(1, num_augmentations + 1):\n",
    "            transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "            aug_file_name = f\"{Path(img_info['file_name']).stem}_aug-{i}{Path(img_info['file_name']).suffix}\"\n",
    "            aug_path = os.path.join(output_image_folder_path, aug_file_name)\n",
    "\n",
    "            cv2.imwrite(aug_path, transformed['image'])\n",
    "\n",
    "            new_images.append({\n",
    "                \"id\": new_image_id,\n",
    "                \"file_name\": aug_file_name,\n",
    "                \"width\": img_info['width'],\n",
    "                \"height\": img_info['height']\n",
    "            })\n",
    "\n",
    "            for bbox, cat_id in zip(transformed['bboxes'], transformed['category_ids']):\n",
    "                new_annotations.append({\n",
    "                    \"id\": new_annotation_id,\n",
    "                    \"image_id\": new_image_id,\n",
    "                    \"category_id\": cat_id,\n",
    "                    \"bbox\": bbox,\n",
    "                    \"iscrowd\": 0,\n",
    "                    \"area\": bbox[2] * bbox[3]\n",
    "                })\n",
    "                new_annotation_id += 1\n",
    "\n",
    "            new_image_id += 1\n",
    "\n",
    "    augmented_coco = {\n",
    "        \"images\": new_images,\n",
    "        \"annotations\": new_annotations,\n",
    "        \"categories\": coco['categories']\n",
    "    }\n",
    "\n",
    "    # Create output folder for JSON if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(augmented_coco, f, indent=2)\n",
    "\n",
    "    print(\"Data augmentation complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b7cf819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jose\\AppData\\Roaming\\Python\\Python312\\site-packages\\albumentations\\core\\validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "Augmenting images: 100%|██████████| 1160/1160 [07:21<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation complete and saved.\n"
     ]
    }
   ],
   "source": [
    "create_subset_data_augmentation(\n",
    "    image_folder_path='merged-dataset/images',\n",
    "    coco_json_path='merged-dataset/subsets/train.json',\n",
    "    output_image_folder_path='merged-dataset/train_augmented/images',\n",
    "    output_json_path='merged-dataset/train_augmented/train_augmented.json',\n",
    "    num_augmentations=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d20e9",
   "metadata": {},
   "source": [
    "#### ii. Train unido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9905101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff2e0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset_data_augmentation_unified(\n",
    "    image_folder_path,\n",
    "    coco_json_path,\n",
    "    output_image_folder_path,\n",
    "    output_json_path,\n",
    "    num_augmentations\n",
    "):\n",
    "    os.makedirs(output_image_folder_path, exist_ok=True)\n",
    "\n",
    "    with open(coco_json_path, 'r', encoding='utf-8') as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    transform = A.Compose([\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.9),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "        A.MotionBlur(blur_limit=3, p=0.3)\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))\n",
    "\n",
    "    new_images = []\n",
    "    new_annotations = []\n",
    "    new_image_id = max(img['id'] for img in coco['images']) + 1\n",
    "    new_annotation_id = max(ann['id'] for ann in coco['annotations']) + 1\n",
    "\n",
    "    image_id_to_anns = {}\n",
    "    for ann in coco['annotations']:\n",
    "        image_id_to_anns.setdefault(ann['image_id'], []).append(ann)\n",
    "\n",
    "    # Save original images and annotations\n",
    "    for img_info in tqdm(coco['images'], desc=\"Copying original images and annotations\"):\n",
    "        orig_img_path = os.path.join(image_folder_path, img_info['file_name'])\n",
    "        dest_img_path = os.path.join(output_image_folder_path, img_info['file_name'])\n",
    "\n",
    "        if not os.path.exists(dest_img_path):\n",
    "            shutil.copy(orig_img_path, dest_img_path)\n",
    "\n",
    "        new_images.append({\n",
    "            \"id\": img_info['id'],\n",
    "            \"file_name\": img_info['file_name'],\n",
    "            \"width\": img_info['width'],\n",
    "            \"height\": img_info['height']\n",
    "        })\n",
    "\n",
    "        for ann in image_id_to_anns.get(img_info['id'], []):\n",
    "            new_annotations.append({\n",
    "                \"id\": ann['id'],\n",
    "                \"image_id\": ann['image_id'],\n",
    "                \"category_id\": ann['category_id'],\n",
    "                \"bbox\": ann['bbox'],\n",
    "                \"iscrowd\": ann.get('iscrowd', 0),\n",
    "                \"area\": ann['bbox'][2] * ann['bbox'][3]\n",
    "            })\n",
    "\n",
    "    # Augment\n",
    "    for img_info in tqdm(coco['images'], desc=\"Applying augmentations\"):\n",
    "        img_path = os.path.join(image_folder_path, img_info['file_name'])\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        anns = image_id_to_anns.get(img_info['id'], [])\n",
    "        if not anns:\n",
    "            continue\n",
    "\n",
    "        img_w, img_h = img_info['width'], img_info['height']\n",
    "\n",
    "        # Convert COCO [x, y, w, h] to YOLO normalized format\n",
    "        yolo_bboxes = []\n",
    "        category_ids = []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            x_center = (x + w / 2) / img_w\n",
    "            y_center = (y + h / 2) / img_h\n",
    "            w_norm = w / img_w\n",
    "            h_norm = h / img_h\n",
    "\n",
    "            def clamp01(v): return max(0.0, min(1.0, v))\n",
    "\n",
    "            yolo_bboxes.append([\n",
    "                clamp01(x_center),\n",
    "                clamp01(y_center),\n",
    "                clamp01(w_norm),\n",
    "                clamp01(h_norm)\n",
    "            ])\n",
    "            category_ids.append(ann['category_id'])\n",
    "\n",
    "        for i in range(1, num_augmentations + 1):\n",
    "            try:\n",
    "                transformed = transform(image=image, bboxes=yolo_bboxes, category_ids=category_ids)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping augmentation for {img_info['file_name']} due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "            aug_file_name = f\"{Path(img_info['file_name']).stem}_aug-{i}{Path(img_info['file_name']).suffix}\"\n",
    "            aug_path = os.path.join(output_image_folder_path, aug_file_name)\n",
    "            cv2.imwrite(aug_path, transformed['image'])\n",
    "\n",
    "            new_images.append({\n",
    "                \"id\": new_image_id,\n",
    "                \"file_name\": aug_file_name,\n",
    "                \"width\": img_info['width'],\n",
    "                \"height\": img_info['height']\n",
    "            })\n",
    "\n",
    "            for bbox, cat_id in zip(transformed['bboxes'], transformed['category_ids']):\n",
    "                x_center, y_center, w_norm, h_norm = bbox\n",
    "\n",
    "                w = w_norm * img_w\n",
    "                h = h_norm * img_h\n",
    "                x = x_center * img_w - w / 2\n",
    "                y = y_center * img_h - h / 2\n",
    "\n",
    "                # Clamp bbox to image bounds\n",
    "                x = max(0, min(x, img_w - 1))\n",
    "                y = max(0, min(y, img_h - 1))\n",
    "                w = max(1, min(w, img_w - x))\n",
    "                h = max(1, min(h, img_h - y))\n",
    "\n",
    "                new_annotations.append({\n",
    "                    \"id\": new_annotation_id,\n",
    "                    \"image_id\": new_image_id,\n",
    "                    \"category_id\": cat_id,\n",
    "                    \"bbox\": [x, y, w, h],\n",
    "                    \"iscrowd\": 0,\n",
    "                    \"area\": w * h\n",
    "                })\n",
    "                new_annotation_id += 1\n",
    "\n",
    "            new_image_id += 1\n",
    "\n",
    "    final_coco = {\n",
    "        \"images\": new_images,\n",
    "        \"annotations\": new_annotations,\n",
    "        \"categories\": coco['categories']\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_coco, f, indent=2)\n",
    "\n",
    "    print(\"Original and augmented images with annotations saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1435ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying original images and annotations: 100%|██████████| 1160/1160 [00:56<00:00, 20.68it/s]\n",
      "Applying augmentations: 100%|██████████| 1160/1160 [06:40<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original and augmented images with annotations saved.\n"
     ]
    }
   ],
   "source": [
    "create_subset_data_augmentation_unified(\n",
    "    image_folder_path='merged-dataset/redim/train',\n",
    "    coco_json_path='merged-dataset/redim/train.json',\n",
    "    output_image_folder_path='merged-dataset/redim/train_augmented',\n",
    "    output_json_path='merged-dataset/redim/train_augmented.json',\n",
    "    num_augmentations=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f7916",
   "metadata": {},
   "source": [
    "#### ii. Agregacion de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f516feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "\n",
    "def create_subset_data_augmentation(\n",
    "    image_folder_path,\n",
    "    coco_json_path,\n",
    "    output_image_folder_path,\n",
    "    output_json_path,\n",
    "    num_augmentations\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies data augmentation to new images and appends new annotations to an existing COCO JSON.\n",
    "\n",
    "    Parameters:\n",
    "        image_folder_path (str): Path to the original image folder.\n",
    "        coco_json_path (str): Path to the original COCO JSON annotations.\n",
    "        output_image_folder_path (str): Directory to save augmented images.\n",
    "        output_json_path (str): Path to save the COCO JSON file (appended, not overwritten).\n",
    "        num_augmentations (int): Number of augmentations to apply per image.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_image_folder_path, exist_ok=True)\n",
    "\n",
    "    # Load original COCO annotations\n",
    "    with open(coco_json_path, 'r', encoding='utf-8') as f:\n",
    "        original_coco = json.load(f)\n",
    "\n",
    "    # Load existing augmented JSON if exists\n",
    "    if os.path.exists(output_json_path):\n",
    "        with open(output_json_path, 'r', encoding='utf-8') as f:\n",
    "            output_coco = json.load(f)\n",
    "        existing_augmented_names = {img['file_name'] for img in output_coco['images']}\n",
    "        new_images = output_coco['images']\n",
    "        new_annotations = output_coco['annotations']\n",
    "        new_image_id = max(img['id'] for img in new_images) + 1\n",
    "        new_annotation_id = max(ann['id'] for ann in new_annotations) + 1\n",
    "    else:\n",
    "        existing_augmented_names = set()\n",
    "        new_images = []\n",
    "        new_annotations = []\n",
    "        new_image_id = max(img['id'] for img in original_coco['images']) + 1\n",
    "        new_annotation_id = max(ann['id'] for ann in original_coco['annotations']) + 1\n",
    "\n",
    "    transform = A.Compose([\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.9),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "        A.MotionBlur(blur_limit=3, p=0.3)\n",
    "    ], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "\n",
    "    image_id_to_anns = {}\n",
    "    for ann in original_coco['annotations']:\n",
    "        image_id_to_anns.setdefault(ann['image_id'], []).append(ann)\n",
    "\n",
    "    for img_info in tqdm(original_coco['images'], desc=\"Checking images for augmentation\"):\n",
    "        img_path = os.path.join(image_folder_path, img_info['file_name'])\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        anns = image_id_to_anns.get(img_info['id'], [])\n",
    "        bboxes = [ann['bbox'] for ann in anns]\n",
    "        category_ids = [ann['category_id'] for ann in anns]\n",
    "\n",
    "        if not bboxes:\n",
    "            continue\n",
    "\n",
    "        base_name = Path(img_info['file_name']).stem\n",
    "        suffix = Path(img_info['file_name']).suffix\n",
    "\n",
    "        for i in range(1, num_augmentations + 1):\n",
    "            aug_file_name = f\"{base_name}_aug-{i}{suffix}\"\n",
    "            if aug_file_name in existing_augmented_names:\n",
    "                continue  # Skip already existing augmentations\n",
    "\n",
    "            transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "            aug_path = os.path.join(output_image_folder_path, aug_file_name)\n",
    "            cv2.imwrite(aug_path, transformed['image'])\n",
    "\n",
    "            new_images.append({\n",
    "                \"id\": new_image_id,\n",
    "                \"file_name\": aug_file_name,\n",
    "                \"width\": img_info['width'],\n",
    "                \"height\": img_info['height']\n",
    "            })\n",
    "\n",
    "            for bbox, cat_id in zip(transformed['bboxes'], transformed['category_ids']):\n",
    "                new_annotations.append({\n",
    "                    \"id\": new_annotation_id,\n",
    "                    \"image_id\": new_image_id,\n",
    "                    \"category_id\": cat_id,\n",
    "                    \"bbox\": bbox,\n",
    "                    \"iscrowd\": 0,\n",
    "                    \"area\": bbox[2] * bbox[3]\n",
    "                })\n",
    "                new_annotation_id += 1\n",
    "\n",
    "            new_image_id += 1\n",
    "\n",
    "    final_coco = {\n",
    "        \"images\": new_images,\n",
    "        \"annotations\": new_annotations,\n",
    "        \"categories\": original_coco['categories']\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_coco, f, indent=2)\n",
    "\n",
    "    print(\"Data augmentation complete. New annotations appended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a835e949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jose\\AppData\\Roaming\\Python\\Python312\\site-packages\\albumentations\\core\\validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "Checking images for augmentation: 100%|██████████| 1160/1160 [00:00<00:00, 1287.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation complete. New annotations appended.\n"
     ]
    }
   ],
   "source": [
    "create_subset_data_augmentation(\n",
    "    image_folder_path='merged-dataset/missing_images',\n",
    "    coco_json_path='merged-dataset/subsets/train.json',\n",
    "    output_image_folder_path='merged-dataset/train_augmented/images',\n",
    "    output_json_path='merged-dataset/train_augmented/train_augmented.json',\n",
    "    num_augmentations=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0210eb2",
   "metadata": {},
   "source": [
    "### 5.2 Verificacion de BBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b9dd1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "from pycocotools.coco import COCO\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "225fd97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_coco_bboxes_to_images(image_dir, annotation_path, output_dir='output', max_images=None):\n",
    "    \"\"\"\n",
    "    Save images with bounding boxes drawn from COCO annotations.\n",
    "\n",
    "    Parameters:\n",
    "        image_dir (str): Directory containing the images.\n",
    "        annotation_path (str): Full path to the COCO annotation file.\n",
    "        output_dir (str): Directory to save output images with drawn bounding boxes.\n",
    "        max_images (int, optional): If provided, randomly selects up to this number of images (without repetition).\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    coco = COCO(annotation_path)\n",
    "    image_ids = coco.getImgIds()\n",
    "\n",
    "    if max_images is not None and max_images < len(image_ids):\n",
    "        image_ids = random.sample(image_ids, max_images)\n",
    "\n",
    "    for img_id in image_ids:\n",
    "        img_data = coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(image_dir, img_data['file_name'])\n",
    "        output_path = os.path.join(output_dir, img_data['file_name'])\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"[WARNING] Image file not found: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            draw.rectangle([x, y, x + w, y + h], outline='red', width=3)\n",
    "\n",
    "        image.save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c989105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "save_coco_bboxes_to_images(\n",
    "    image_dir='merged-dataset/train_augmented/images',\n",
    "    annotation_path='merged-dataset/train_augmented/train_augmented.json',\n",
    "    output_dir='merged-dataset/train_augmented/images_bbox_test',\n",
    "    max_images=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ef6f1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "save_coco_bboxes_to_images(\n",
    "    image_dir='merged-dataset/redim/train_augmented',\n",
    "    annotation_path='merged-dataset/redim/train_augmented.json',\n",
    "    output_dir='merged-dataset/redim/train_augmented_bbox_test',\n",
    "    max_images=40\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd155c",
   "metadata": {},
   "source": [
    "### 5.3 Cantidad de Imagenes COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650bef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def count_images_in_coco(coco_json_path):\n",
    "    \"\"\"\n",
    "    Reads a COCO-format JSON file and prints the total number of images.\n",
    "\n",
    "    Parameters:\n",
    "        coco_json_path (str): Path to the COCO JSON annotation file.\n",
    "    \"\"\"\n",
    "    with open(coco_json_path, 'r', encoding='utf-8') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    num_images = len(coco_data.get(\"images\", []))\n",
    "    print(f\"Total number of images: {num_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7fa3c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 1160\n"
     ]
    }
   ],
   "source": [
    "count_images_in_coco('merged-dataset/subsets/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1929560c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 185\n"
     ]
    }
   ],
   "source": [
    "count_images_in_coco('merged-dataset/redim/test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5cd3c",
   "metadata": {},
   "source": [
    "### 5.4 Cantidad e Imagenes Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70f44e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_images_in_folder(folder_path, recursive=False):\n",
    "    \"\"\"\n",
    "    Counts and displays the total number of image files in a folder.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path to the folder.\n",
    "        recursive (bool): If True, searches subdirectories as well.\n",
    "    \"\"\"\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp', '.gif'}\n",
    "    count = 0\n",
    "\n",
    "    if recursive:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if os.path.splitext(file)[1].lower() in image_extensions:\n",
    "                    count += 1\n",
    "    else:\n",
    "        for file in os.listdir(folder_path):\n",
    "            if os.path.isfile(os.path.join(folder_path, file)) and \\\n",
    "               os.path.splitext(file)[1].lower() in image_extensions:\n",
    "                count += 1\n",
    "\n",
    "    print(f\"Total image files in folder: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bf0253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image files in folder: 1160\n"
     ]
    }
   ],
   "source": [
    "count_images_in_folder('merged-dataset/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13ac394b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image files in folder: 1160\n"
     ]
    }
   ],
   "source": [
    "count_images_in_folder('merged-dataset/redim/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d68309de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image files in folder: 185\n"
     ]
    }
   ],
   "source": [
    "count_images_in_folder('merged-dataset/redim/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee166467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image files in folder: 5800\n"
     ]
    }
   ],
   "source": [
    "count_images_in_folder('merged-dataset/redim/train_augmented')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2768e77",
   "metadata": {},
   "source": [
    "### 5.5 Verificar aumento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28ff8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_augmented_images(original_path, augmented_path):\n",
    "    \"\"\"\n",
    "    Compares original and augmented images to find:\n",
    "    1. Original images missing some of the 4 required augmented versions.\n",
    "    2. Augmented images with no matching original.\n",
    "\n",
    "    Parameters:\n",
    "        original_path (str): Path to folder containing original images.\n",
    "        augmented_path (str): Path to folder containing augmented images.\n",
    "    \"\"\"\n",
    "    # Get original base names without extensions\n",
    "    original_bases = {\n",
    "        os.path.splitext(f)[0] for f in os.listdir(original_path)\n",
    "        if os.path.isfile(os.path.join(original_path, f))\n",
    "    }\n",
    "\n",
    "    # Mapping from original base name to found suffixes\n",
    "    aug_suffixes = {\"_aug-1\", \"_aug-2\", \"_aug-3\", \"_aug-4\"}\n",
    "    aug_tracking = {base: set() for base in original_bases}\n",
    "    unmatched_augmented = []\n",
    "\n",
    "    for f in os.listdir(augmented_path):\n",
    "        name, _ = os.path.splitext(f)\n",
    "        matched = False\n",
    "        for suffix in aug_suffixes:\n",
    "            if name.endswith(suffix):\n",
    "                base_name = name.replace(suffix, \"\")\n",
    "                if base_name in original_bases:\n",
    "                    aug_tracking[base_name].add(suffix)\n",
    "                    matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            unmatched_augmented.append(f)\n",
    "\n",
    "    print(\"Images missing augmented versions:\")\n",
    "    for base, suffixes in aug_tracking.items():\n",
    "        missing = aug_suffixes - suffixes\n",
    "        if missing:\n",
    "            print(f\"- {base}: missing {sorted(missing)}\")\n",
    "\n",
    "    print(\"\\nAugmented images with no matching original:\")\n",
    "    for aug in unmatched_augmented:\n",
    "        print(f\"- {aug}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2370ccd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images missing augmented versions:\n",
      "\n",
      "Augmented images with no matching original:\n"
     ]
    }
   ],
   "source": [
    "check_augmented_images(\"merged-dataset/train\", \"merged-dataset/train_augmented/images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
