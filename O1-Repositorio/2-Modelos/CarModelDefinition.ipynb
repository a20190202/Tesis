{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac1b71f",
   "metadata": {},
   "source": [
    "# Definicion de 50 Modelos de Automoviles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24960240",
   "metadata": {},
   "source": [
    "## 1. Modelos de las marcas mas vendidas en Peru\n",
    "A partir de un dataset publico \"us-car-models-data-master\", se filtraron los modelos de acuerdo a la marca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c46aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directorio con los archivos CSV por año\n",
    "csv_directory = r'us-car-models-data-master'\n",
    "\n",
    "# Lista de marcas seleccionadas (en minúsculas)\n",
    "selected_makes = [\n",
    "    'toyota', 'hyundai', 'kia', 'chevrolet', 'changan',\n",
    "    'nissan', 'suzuki', 'dfsk', 'jac', 'volkswagen',\n",
    "    'mitsubishi', 'ford', 'chery', 'mazda', 'renault',\n",
    "    'honda', 'great wall', 'subaru', 'mg', 'foton'\n",
    "]\n",
    "\n",
    "# DataFrame para acumular datos\n",
    "filtered_data = pd.DataFrame()\n",
    "\n",
    "# Leer y filtrar archivos por año\n",
    "for year in range(1992, 2025):\n",
    "    csv_file = os.path.join(csv_directory, f'{year}.csv')\n",
    "    \n",
    "    if os.path.exists(csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['make'] = df['make'].str.lower()\n",
    "        df_filtered = df[df['make'].isin(selected_makes)].copy()\n",
    "        df_filtered['year'] = year  # Agregar columna del año\n",
    "        filtered_data = pd.concat([filtered_data, df_filtered], ignore_index=True)\n",
    "\n",
    "# Guardar los datos filtrados en CSV\n",
    "filtered_data.to_csv('new_models.csv', index=False)\n",
    "\n",
    "# Crear resumen por marca\n",
    "summary = (\n",
    "    filtered_data\n",
    "    .groupby('make')\n",
    "    .agg(\n",
    "        cantidad_modelos=('model', 'nunique'),\n",
    "        años=('year', lambda x: ','.join(map(str, sorted(set(x)))))\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={'make': 'marca'})\n",
    ")\n",
    "\n",
    "# Guardar el resumen en un archivo Excel\n",
    "summary.to_excel('resumen_marcas.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f0732",
   "metadata": {},
   "source": [
    "## 2. Pytrends para comparar el interes de los modelos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38840126",
   "metadata": {},
   "source": [
    "### 2.1 Generacion de Archivos Excel con los intereses en el tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "# Initialize Pytrends\n",
    "pytrends = TrendReq(hl='es-PE')  # Set the region to Peru\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "df= pd.read_csv('models.csv')\n",
    "makes = df['make'].unique().tolist()\n",
    "models = df[['make', 'model']].values.tolist()\n",
    "models = [' '.join(make_model) for make_model in models]\n",
    "new_models = [models[102]] + models[:102] + models[102:]  #'toyota Corolla' : models[102]\n",
    "new_models = list(dict.fromkeys(new_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos_faltantes = ['changan X7 Plus', 'changan UNI-K', 'changan UNI-T', 'changan New CS55 Plus', 'changan New CS15', 'changan New CS35 Plus', 'changan New Van', 'changan HONOR S', 'changan Grand SUPERVAN', 'changan Grand VAN Turismo', 'changan New Alsvin', 'changan NEW F70',\n",
    "'dfsk Glory 500', 'dfsk Glory iX5', 'dfsk DFSK E5', 'dfsk Glory 600', 'dfsk Glory 560', 'dfsk Glory 580', 'dfsk Glory 330', 'dfsk C37', 'dfsk K07S', 'dfsk K05S Cargo Panel', 'dfsk C31 Cabina Simple', 'dfsk C35 Cargo Panel',\n",
    "'jac JS2', 'jac JS3', 'jac JS4', 'jac JS6', 'jac JS8', 'jac T6', 'jac T8', 'jac T8 PRO', 'jac Nueva Refine', 'jac Sunray', 'jac X200', 'jac E-JS1', 'jac E-JS4', 'jac E-T8', 'jac E-M3',\n",
    "'renault New Kwid', 'renault Stepway', 'renault All New Duster', 'renault New Renault Kardian', 'renault New Koleos', 'renault New Kwid E Tech', 'renault New Oroch', 'renault Master Furgón', 'renault Master Minibus',\n",
    "'chery Tiggo 8 Pro Hybrid', 'chery Tiggo 8 Pro Max', 'chery Tiggo 8 Pro', 'chery Tiggo 8', 'chery Tiggo 7 Pro Hybrid', 'chery Tiggo 7 Pro', 'chery Tiggo 4 Pro', 'chery Tiggo 2 Pro', 'chery Arrizo 5',\n",
    "'jetour X70', 'jetour X70Plus', 'jetour Dashing', 'jetour X90Plus', 'jetour Traveller T2',\n",
    "'geely GX3 Pro', 'geely New Coolray', 'geely New Okavango', 'geely New Starray', 'geely Emgrand',\n",
    "'great wall H6 GT', 'great wall DARGO', 'great wall H6', 'great wall JOLION', 'great wall POER AT', 'great wall POER MT', 'great wall WINGLE 7 DIESEL', 'great wall WINGLE 5 DIESEL', 'great wall WINGLE 5 GASOLINA', 'great wall H6 HEV', 'great wall JOLION HEV',\n",
    "'mg ZX', 'mg ZX PLUS', 'mg HS', 'mg RX5', 'mg ONE', 'mg RX8', 'mg 5', 'mg 6', 'mg GT', 'mg 4', 'mg 3', 'mg 3 HYBRID+',\n",
    "'jinbei F50', 'jinbei H2L', 'jinbei Haise', 'jinbei Konect',\n",
    "'foton K0', 'foton K1', 'foton K2', 'foton BigVan Toano', 'foton Tunland G7 4x4', 'foton Tunland G7 4x2', 'foton Tunland G9 4x4', 'foton Tunland G9 4x4', 'foton Tunland G9 DS']\n",
    "\n",
    "new_list = [new_models[0]] + modelos_faltantes\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba05236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize variables\n",
    "data = []\n",
    "previous_df = pd.DataFrame()\n",
    "\n",
    "def calculate_means(chunk):\n",
    "    pytrends.build_payload(chunk, cat=0, timeframe='today 5-y', geo='PE')\n",
    "    data = pytrends.interest_over_time()\n",
    "    return data.mean()\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "for chunk_number, chunk in enumerate([lst + [new_list[0]] for lst in list(chunks(new_list[1:], 4))]):\n",
    "    if(chunk_number < 83): continue\n",
    "    try:\n",
    "        means = calculate_means(chunk)\n",
    "\n",
    "        # Display relevant details in the console\n",
    "        print(f\"Processing chunk {chunk_number}\")\n",
    "        print(\"Chunk models:\", chunk)\n",
    "\n",
    "        # Save the current chunk's data\n",
    "        current_data = []\n",
    "        for i, model in enumerate(chunk):\n",
    "            is_the_same_value = model == 'toyota Corolla'\n",
    "            current_data.append([chunk_number, is_the_same_value, model, means[i]])\n",
    "\n",
    "        current_df = pd.DataFrame(current_data, columns=['chunk_number', 'is_the_same_value', 'model', 'value'])\n",
    "\n",
    "        # Save current and previous dataframes to Excel\n",
    "        with pd.ExcelWriter(f'chunk_data_rest_{chunk_number}.xlsx') as writer:\n",
    "            current_df.to_excel(writer, sheet_name='Current', index=False)\n",
    "            previous_df.to_excel(writer, sheet_name='Previous', index=False)\n",
    "\n",
    "        # Update the previous_df to be the current one for the next iteration\n",
    "        previous_df = current_df.copy()\n",
    "\n",
    "        time.sleep(60)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {chunk_number}: {e}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd6efe",
   "metadata": {},
   "source": [
    "### 2.2 Calculo de la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbfb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the Excel files\n",
    "directory = './respaldo/bkup1/'\n",
    "\n",
    "# List to store the filtered values\n",
    "all_values = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        # Extract chunk_number from the filename using regex\n",
    "        match = re.search(r'chunk_data(?:_rest)?_(\\d+)\\.xlsx', filename)\n",
    "        if match:\n",
    "            chunk_number = int(match.group(1))\n",
    "        print(chunk_number)\n",
    "        # Read the Excel file\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_excel(filepath)\n",
    "        \n",
    "        # Filter rows where is_the_same_value is True\n",
    "        filtered_df = df[df['is_the_same_value'] == True]\n",
    "        \n",
    "        # Append the filtered values to the list\n",
    "        all_values.extend(filtered_df['value'].tolist())\n",
    "\n",
    "# Calculate the mean of all filtered values\n",
    "value_mean = pd.Series(all_values).mean()\n",
    "\n",
    "# Display value_mean\n",
    "print(value_mean)\n",
    "\n",
    "# Save value_mean to a text file\n",
    "with open('value_mean.txt', 'w') as file:\n",
    "    file.write(str(value_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad3c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Directory containing the Excel files\n",
    "directory = './respaldo/bkup1/'\n",
    "\n",
    "# List to store the filtered values\n",
    "all_values = []\n",
    "\n",
    "# Dictionary to store the mean value for each chunk\n",
    "chunk_means = {}\n",
    "\n",
    "#read the value_mean from the text file\n",
    "with open('value_mean.txt', 'r') as file:\n",
    "    mean_value = float(file.read())\n",
    "\n",
    "# List to store the updated DataFrames\n",
    "updated_dfs = []\n",
    "\n",
    "# Second pass: Update the values and store the updated DataFrames\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        # Extract chunk_number from the filename using regex\n",
    "        match = re.search(r'chunk_data(?:_rest)?_(\\d+)\\.xlsx', filename)\n",
    "        if match:\n",
    "            chunk_number = int(match.group(1))\n",
    "        \n",
    "        # Read the Excel file\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_excel(filepath)\n",
    "         \n",
    "        \n",
    "        # Get the value of the row where is_the_same_value is True\n",
    "        true_value = df[df['is_the_same_value'] == True]['value'].values[0]\n",
    "        \n",
    "        # Update the values\n",
    "        df['value'] = df['value'] * mean_value / true_value\n",
    "        \n",
    "        # Append the updated DataFrame to the list\n",
    "        updated_dfs.append(df)\n",
    "\n",
    "# Concatenate all the updated DataFrames\n",
    "result_df = pd.concat(updated_dfs, ignore_index=True)\n",
    "\n",
    "# Save the result to a new Excel file\n",
    "result_df.to_excel('updated_values.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e521d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from update_values, give me the top 50 with the highest value and the model name is not repeated\n",
    "\n",
    "result_df = pd.read_excel('updated_values.xlsx')\n",
    "\n",
    "# Sort the DataFrame by value in descending order\n",
    "sorted_df = result_df.sort_values(by='value', ascending=False)\n",
    "\n",
    "# Drop duplicates based on the model column, keeping the first occurrence\n",
    "unique_models_df = sorted_df.drop_duplicates(subset='model', keep='first')\n",
    "\n",
    "# Select the top 50 rows\n",
    "top_50_df = unique_models_df.head(50)\n",
    "\n",
    "# Save the top 50 rows to a new Excel file\n",
    "top_50_df.to_excel('top_50_values.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
