{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e5de20",
   "metadata": {},
   "source": [
    "# Preprocesamiento I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f60d0",
   "metadata": {},
   "source": [
    "## 1. Lectura de archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b4966",
   "metadata": {},
   "source": [
    "### 1.1 Module 1: read_images.py\n",
    "\n",
    "Handles reading image names from the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_image_names(folders):\n",
    "    \"\"\"\n",
    "    Reads the image names and extensions from test, train, and valid subfolders \n",
    "    for the given list of folders.\n",
    "    \n",
    "    Args:\n",
    "        folders (list): List of folder names to process.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with folder names as keys and sub-dictionaries \n",
    "              for splits (test, train, valid), each containing a set of image names.\n",
    "    \"\"\"\n",
    "    image_data = {folder: {'test': set(), 'train': set(), 'valid': set()} for folder in folders}\n",
    "    \n",
    "    for folder in folders:\n",
    "        for split in ['test', 'train', 'valid']:\n",
    "            split_path = os.path.join(folder, split, 'images')\n",
    "            if os.path.exists(split_path):\n",
    "                image_data[folder][split].update(os.listdir(split_path))\n",
    "    \n",
    "    return image_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4663c1",
   "metadata": {},
   "source": [
    "### 1.2 Module 2: count_images.py\n",
    "\n",
    "Counts the number of images in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b3e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(image_data):\n",
    "    \"\"\"\n",
    "    Prints the count of images in each split for each folder.\n",
    "\n",
    "    Args:\n",
    "        image_data (dict): Dictionary containing image data organized by folders and splits.\n",
    "    \"\"\"\n",
    "    for folder, splits in image_data.items():\n",
    "        print(f\"\\nFolder: {folder}\")\n",
    "        for split, images in splits.items():\n",
    "            print(f\"  {split.capitalize()}: {len(images)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8317dd",
   "metadata": {},
   "source": [
    "### 1.3 Module 3: find_repeated.py\n",
    "\n",
    "Finds image names that are repeated across folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e4725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repeated_images(image_data):\n",
    "    \"\"\"\n",
    "    Finds image names that are repeated across folders.\n",
    "\n",
    "    Args:\n",
    "        image_data (dict): Dictionary containing image data organized by folders and splits.\n",
    "\n",
    "    Returns:\n",
    "        set: Set of image names that are repeated across folders.\n",
    "    \"\"\"\n",
    "    all_images = {}\n",
    "    \n",
    "    for folder, splits in image_data.items():\n",
    "        for split, images in splits.items():\n",
    "            for image in images:\n",
    "                if image not in all_images:\n",
    "                    all_images[image] = 0\n",
    "                all_images[image] += 1\n",
    "    \n",
    "    repeated_images = {image for image, count in all_images.items() if count > 1}\n",
    "    return repeated_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f5494",
   "metadata": {},
   "source": [
    "### 1.4 Module 4: unique_train_images.py\n",
    "\n",
    "Finds unique image names in the train split for each folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384eb248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_train_images(image_data):\n",
    "    \"\"\"\n",
    "    Finds unique image names for the `train` split of each folder.\n",
    "\n",
    "    Args:\n",
    "        image_data (dict): Dictionary containing image data organized by folders and splits.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with folder names as keys and sets of unique image names as values.\n",
    "    \"\"\"\n",
    "    unique_images = {}\n",
    "    all_train_images = set()\n",
    "\n",
    "    # Collect all train images from all folders\n",
    "    for folder, splits in image_data.items():\n",
    "        all_train_images.update(splits['train'])\n",
    "\n",
    "    # Identify unique images for each folder\n",
    "    for folder, splits in image_data.items():\n",
    "        unique_images[folder] = splits['train'] - (all_train_images - splits['train'])\n",
    "    \n",
    "    return unique_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd837a",
   "metadata": {},
   "source": [
    "### 1.5 Module 5: display_extensions.py\n",
    "\n",
    "Displays the file extensions of images in the test, train, and valid splits for each folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def display_image_extensions(image_data):\n",
    "    \"\"\"\n",
    "    Displays the unique file extensions of images in test, train, and valid splits for each folder.\n",
    "\n",
    "    Args:\n",
    "        image_data (dict): Dictionary containing image data organized by folders and splits.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with folder names as keys and sub-dictionaries containing unique extensions for each split.\n",
    "    \"\"\"\n",
    "    extensions = {folder: {'test': set(), 'train': set(), 'valid': set()} for folder in image_data.keys()}\n",
    "    \n",
    "    for folder, splits in image_data.items():\n",
    "        for split, images in splits.items():\n",
    "            for image in images:\n",
    "                ext = os.path.splitext(image)[1].lower()  # Get extension\n",
    "                extensions[folder][split].add(ext)\n",
    "    \n",
    "    return extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739108a",
   "metadata": {},
   "source": [
    "### 1.6 Module 6: display_dimensions.py\n",
    "\n",
    "Displays the dimensions of images in the test, train, and valid splits for each folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def display_unique_dimensions(folders):\n",
    "    \"\"\"\n",
    "    Displays the unique dimensions of images in test, train, and valid splits for each folder.\n",
    "\n",
    "    Args:\n",
    "        folders (list): List of folder names to process.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with folder names as keys and sub-dictionaries for splits \n",
    "              (test, train, valid), each containing a set of unique image dimensions.\n",
    "    \"\"\"\n",
    "    unique_dimensions = {folder: {'test': set(), 'train': set(), 'valid': set()} for folder in folders}\n",
    "    \n",
    "    for folder in folders:\n",
    "        for split in ['test', 'train', 'valid']:\n",
    "            split_path = os.path.join(folder, split, 'images')\n",
    "            if os.path.exists(split_path):\n",
    "                for image in os.listdir(split_path):\n",
    "                    image_path = os.path.join(split_path, image)\n",
    "                    try:\n",
    "                        with Image.open(image_path) as img:\n",
    "                            unique_dimensions[folder][split].add(img.size)  # Add (width, height) to the set\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not read dimensions for {image_path}: {e}\")\n",
    "    \n",
    "    return unique_dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a9c69",
   "metadata": {},
   "source": [
    "### 1.7 Main Script: main.py\n",
    "\n",
    "Combine all modules and execute the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27955b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Define folders\n",
    "    folders = [\n",
    "        'ANPR2.v1i.yolov8',\n",
    "        'NumberPlates.v1i.yolov8',\n",
    "        'Peru License Plate.v7i.yolov8',\n",
    "        'Peru Plate Numbers.v3i.yolov8'\n",
    "    ]\n",
    "\n",
    "    # Step 2: Read image names\n",
    "    image_data = read_image_names(folders)\n",
    "\n",
    "    # Step 3: Display image counts\n",
    "    print(\"Image counts:\")\n",
    "    count_images(image_data)\n",
    "\n",
    "    # Step 4: Find repeated images\n",
    "    repeated_images = find_repeated_images(image_data)\n",
    "    print(f\"\\nRepeated images across folders: {len(repeated_images)}\")\n",
    "    print(repeated_images)\n",
    "\n",
    "    # Step 5: Find unique train images\n",
    "    unique_train_images = find_unique_train_images(image_data)\n",
    "    print(\"\\nUnique train images per folder:\")\n",
    "    for folder, unique_images in unique_train_images.items():\n",
    "        print(f\"  {folder}: {len(unique_images)} unique images\")\n",
    "\n",
    "\n",
    "    # Step 6: Display image extensions\n",
    "    print(\"\\nImage extensions:\")\n",
    "    extensions = display_image_extensions(image_data)\n",
    "    for folder, splits in extensions.items():\n",
    "        print(f\"Folder: {folder}\")\n",
    "        for split, ext_set in splits.items():\n",
    "            print(f\"  {split.capitalize()}: {ext_set}\")\n",
    "\n",
    "    # Step 7: Display unique image dimensions\n",
    "    print(\"\\nUnique image dimensions:\")\n",
    "    unique_dimensions = display_unique_dimensions(folders)\n",
    "    for folder, splits in unique_dimensions.items():\n",
    "        print(f\"Folder: {folder}\")\n",
    "        for split, dimensions in splits.items():\n",
    "            print(f\"  {split.capitalize()}: {dimensions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7836c",
   "metadata": {},
   "source": [
    "## 2. Limpieza de imagenes con aumento de datos de los datasets publicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ff35f",
   "metadata": {},
   "source": [
    "### 2.1 Dataset \"ANPR2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f7f3b",
   "metadata": {},
   "source": [
    "#### i. Primer filtro de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b23806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing the images\n",
    "image_dir = \"ANPR2.v1i.yolov8/train/images\"\n",
    "filtered_dir = \"ANPR2.v1i.yolov8/train_filter_v2/images\"\n",
    "\n",
    "# Ensure the filtered directory exists\n",
    "os.makedirs(filtered_dir, exist_ok=True)\n",
    "\n",
    "# Dictionary to store the original images\n",
    "original_images = []\n",
    "\n",
    "# Group files by their prefix (common part before \".rf.\")\n",
    "grouped_files = defaultdict(list)\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        prefix = filename.split(\".rf.\")[0]\n",
    "        grouped_files[prefix].append(filename)\n",
    "\n",
    "# Identify the original images\n",
    "for prefix, files in grouped_files.items():\n",
    "    # Sort files to ensure consistent order\n",
    "    files.sort()\n",
    "\n",
    "    # Determine the position of the original image based on the prefix\n",
    "    position = 0  # Default to the first \n",
    "\n",
    "    # Add the original image to the list\n",
    "    if position < len(files):\n",
    "        original_images.append(files[position])\n",
    "\n",
    "# Copy the original images to the filtered directory\n",
    "for img in original_images:\n",
    "    src_path = os.path.join(image_dir, img)\n",
    "    dst_path = os.path.join(filtered_dir, img)\n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n",
    "# Output the original images\n",
    "print(\"Original Images copied to filtered directory:\")\n",
    "for img in original_images:\n",
    "    print(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d04180",
   "metadata": {},
   "source": [
    "#### ii. Stage 2: Depuracion de imagnes no originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c0d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define paths\n",
    "image_dir = \"ANPR2.v1i.yolov8/train/images\"\n",
    "invalid_file = \"limpieza_dataset1_stage1.csv\"\n",
    "output_dir = \"ANPR2.v1i.yolov8/train_stage2_options/images\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 1: Read all image filenames\n",
    "image_files = sorted(os.listdir(image_dir))\n",
    "\n",
    "# Step 2: Load invalid prefixes (from CSV)\n",
    "invalid_prefixes = set()\n",
    "if os.path.exists(invalid_file):\n",
    "    df_invalid = pd.read_csv(invalid_file, header=None)\n",
    "    for filename in df_invalid[0]:\n",
    "        prefix = filename.split(\".rf.\")[0]  # Extract prefix\n",
    "        invalid_prefixes.add(prefix)\n",
    "\n",
    "# Step 3: Group images by prefix\n",
    "grouped_images = defaultdict(list)\n",
    "for image in image_files:\n",
    "    if \".rf.\" in image:\n",
    "        prefix = image.split(\".rf.\")[0]  # Extract the shared prefix\n",
    "        grouped_images[prefix].append(image)\n",
    "\n",
    "# Step 4: Select one of the remaining options for review\n",
    "for prefix, files in grouped_images.items():\n",
    "    if prefix in invalid_prefixes:\n",
    "        remaining_files = [f for f in files if f.split(\".rf.\")[0] == prefix]\n",
    "        \n",
    "        # Select the last one in sorted order (arbitrary choice)\n",
    "        if remaining_files:\n",
    "            selected_file = remaining_files[-1]\n",
    "            \n",
    "            # Copy selected image to train_stage2_options/images\n",
    "            src_path = os.path.join(image_dir, selected_file)\n",
    "            dest_path = os.path.join(output_dir, selected_file)\n",
    "            shutil.copy2(src_path, dest_path)\n",
    "\n",
    "print(\"Images for stage 2 review copied successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f8ffc1",
   "metadata": {},
   "source": [
    "#### iii. Stage 3: Depuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3353269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define paths\n",
    "image_dir = \"ANPR2.v1i.yolov8/train/images\"\n",
    "invalid_file1 = \"limpieza_dataset1_stage1.csv\"\n",
    "invalid_file2 = \"limpieza_dataset_placas_stage-stage2.csv\"\n",
    "output_dir = \"ANPR2.v1i.yolov8/train_stage3_options/images\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 1: Read all image filenames\n",
    "image_files = sorted(os.listdir(image_dir))\n",
    "print(f\"Total images found: {len(image_files)}\")\n",
    "\n",
    "# Step 2: Load invalid prefixes from both CSVs (use prefixes, not full filenames)\n",
    "invalid_prefixes_stage1 = set()\n",
    "if os.path.exists(invalid_file1):\n",
    "    df_invalid1 = pd.read_csv(invalid_file1, header=None)\n",
    "    for filename in df_invalid1[0]:\n",
    "        prefix = filename.split(\".rf.\")[0]  # Extract prefix\n",
    "        invalid_prefixes_stage1.add(prefix)\n",
    "    print(f\"Invalid prefixes from stage 1: {len(invalid_prefixes_stage1)}\")\n",
    "\n",
    "invalid_prefixes_stage2 = set()\n",
    "if os.path.exists(invalid_file2):\n",
    "    df_invalid2 = pd.read_csv(invalid_file2, header=None)\n",
    "    for filename in df_invalid2[0]:\n",
    "        prefix = filename.split(\".rf.\")[0]  # Extract prefix\n",
    "        invalid_prefixes_stage2.add(prefix)\n",
    "    print(f\"Invalid prefixes from stage 2: {len(invalid_prefixes_stage2)}\")\n",
    "\n",
    "# Find prefixes mentioned in both invalid lists\n",
    "common_invalid_prefixes = invalid_prefixes_stage1.intersection(invalid_prefixes_stage2)\n",
    "print(f\"Common invalid prefixes (intersection): {len(common_invalid_prefixes)}\")\n",
    "\n",
    "# Step 3: Group images by prefix\n",
    "grouped_images = defaultdict(list)\n",
    "for image in image_files:\n",
    "    if \".rf.\" in image:\n",
    "        prefix = image.split(\".rf.\")[0]  # Extract the shared prefix\n",
    "        grouped_images[prefix].append(image)\n",
    "\n",
    "print(f\"Total prefixes grouped: {len(grouped_images)}\")\n",
    "\n",
    "# Step 4: Select the only remaining option for each prefix\n",
    "copied_count = 0\n",
    "for prefix in common_invalid_prefixes:\n",
    "    if prefix in grouped_images:\n",
    "        files = grouped_images[prefix]\n",
    "        print(f\"\\nChecking prefix: {prefix}\")\n",
    "        print(f\"Files under prefix {prefix}: {files}\")\n",
    "\n",
    "        # Filter out images that are listed as invalid (by filename)\n",
    "        invalid_fullnames = set(df_invalid1[0].tolist() + df_invalid2[0].tolist())\n",
    "        remaining_files = [f for f in files if f.replace('.jpg', '') not in invalid_fullnames]\n",
    "\n",
    "        print(f\"Remaining files for prefix {prefix}: {remaining_files}\")\n",
    "\n",
    "        # If only one option remains, copy it to output directory\n",
    "        if len(remaining_files) == 1:\n",
    "            selected_file = remaining_files[0]\n",
    "            print(f\"Selected file for prefix {prefix}: {selected_file}\")\n",
    "\n",
    "            src_path = os.path.join(image_dir, selected_file)\n",
    "            dest_path = os.path.join(output_dir, selected_file)\n",
    "            shutil.copy2(src_path, dest_path)\n",
    "            copied_count += 1\n",
    "        else:\n",
    "            print(f\"Multiple or no options found for prefix {prefix}, skipping...\")\n",
    "    else:\n",
    "        print(f\"No files grouped for prefix {prefix}, skipping...\")\n",
    "\n",
    "print(f\"\\nImages copied for stage 3 review: {copied_count}\")\n",
    "print(\"Process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d654123",
   "metadata": {},
   "source": [
    "#### iv. Stage 4: Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff082c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "filter_v2_dir = \"ANPR2.v1i.yolov8/train_filter_v2/images\"\n",
    "stage2_dir = \"ANPR2.v1i.yolov8/train_stage2_options/images\"\n",
    "stage3_dir = \"ANPR2.v1i.yolov8/train_stage3_options/images\"\n",
    "\n",
    "filter_v2_invalid_file = \"limpieza_dataset1_stage1.csv\"\n",
    "stage2_invalid_file = \"limpieza_dataset_placas_stage-stage2.csv\"\n",
    "output_csv = \"original_images.csv\"\n",
    "\n",
    "# Step 1: Helper function to extract prefix (before .rf.)\n",
    "def get_prefix(filename):\n",
    "    return filename.split(\".rf.\")[0]\n",
    "\n",
    "# Step 2: Load invalid prefixes from CSV files (prefixes only)\n",
    "filter_v2_invalid_prefixes = set(pd.read_csv(filter_v2_invalid_file, header=None)[0].apply(get_prefix))\n",
    "print(f\"Invalid prefixes from filter_v2: {len(filter_v2_invalid_prefixes)}\")\n",
    "\n",
    "stage2_invalid_prefixes = set(pd.read_csv(stage2_invalid_file, header=None)[0].apply(get_prefix))\n",
    "print(f\"Invalid prefixes from stage 2: {len(stage2_invalid_prefixes)}\")\n",
    "\n",
    "# Step 3: Get original images from Filter V2\n",
    "filter_v2_images = set(os.listdir(filter_v2_dir))\n",
    "filter_v2_original = {img for img in filter_v2_images if get_prefix(img) not in filter_v2_invalid_prefixes}\n",
    "print(f\"Original images from filter_v2: {len(filter_v2_original)}\")\n",
    "\n",
    "# Step 4: Get original images from Stage 2\n",
    "stage2_images = set(os.listdir(stage2_dir))\n",
    "stage2_original = {img for img in stage2_images if get_prefix(img) not in stage2_invalid_prefixes}\n",
    "print(f\"Original images from stage 2: {len(stage2_original)}\")\n",
    "\n",
    "# Step 5: Get original images from Stage 3 (remaining after subsets 1 and 2)\n",
    "stage3_images = set(os.listdir(stage3_dir))\n",
    "already_accounted_prefixes = {get_prefix(img) for img in filter_v2_original.union(stage2_original)}\n",
    "stage3_original = {img for img in stage3_images if get_prefix(img) not in already_accounted_prefixes}\n",
    "print(f\"Original images from stage 3: {len(stage3_original)}\")\n",
    "\n",
    "# Step 6: Combine all original images\n",
    "all_original_images = sorted(filter_v2_original.union(stage2_original, stage3_original))\n",
    "print(f\"Total original images collected: {len(all_original_images)}\")\n",
    "\n",
    "# Step 7: Save to CSV\n",
    "df_original = pd.DataFrame(all_original_images, columns=[\"original_images\"])\n",
    "df_original.to_csv(output_csv, index=False)\n",
    "print(f\"Original images saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4877394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy paste\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define paths\n",
    "original_images_file = \"original_images.csv\"\n",
    "train_images_dir = \"ANPR2.v1i.yolov8/train/images\"\n",
    "output_dir = \"ANPR2.v1i.yolov8/train_stage4_options/images\"\n",
    "\n",
    "# Step 2: Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 3: Load selected original image filenames from CSV\n",
    "df = pd.read_csv(original_images_file)\n",
    "original_images = set(df[\"original_images\"].tolist())\n",
    "print(f\"Total selected original images: {len(original_images)}\")\n",
    "\n",
    "# Step 4: Copy matching images from train/images to train_stage4_options/images\n",
    "copied_count = 0\n",
    "for image in original_images:\n",
    "    src_path = os.path.join(train_images_dir, image)\n",
    "    dest_path = os.path.join(output_dir, image)\n",
    "\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy2(src_path, dest_path)\n",
    "        copied_count += 1\n",
    "    else:\n",
    "        print(f\"Image not found: {image}\")\n",
    "\n",
    "print(f\"Total images copied: {copied_count}\")\n",
    "print(f\"Images saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a602c",
   "metadata": {},
   "source": [
    "#### v. Eliminacion de imagenes de motos y placas obstruidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484410a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Directories for the images\n",
    "stage6_dir = \"ANPR2.v1i.yolov8/train_stage6_options/images\"\n",
    "filter_dir = \"ANPR2.v1i.yolov8/train_del/train_filter_v2/images\"\n",
    "\n",
    "def get_prefixes(directory):\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping each unique prefix (the part before \".rf\")\n",
    "    to a list of corresponding filenames in the given directory.\n",
    "    \"\"\"\n",
    "    prefix_to_files = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if \".rf\" in filename:\n",
    "            prefix = filename.split(\".rf\")[0]\n",
    "            prefix_to_files.setdefault(prefix, []).append(filename)\n",
    "    return prefix_to_files\n",
    "\n",
    "# Get prefix mappings from both directories\n",
    "stage6_prefixes = get_prefixes(stage6_dir)\n",
    "filter_prefixes = get_prefixes(filter_dir)\n",
    "\n",
    "# Determine unique prefixes in stage6 that are not in filter_dir\n",
    "unique_stage6_prefixes = set(stage6_prefixes.keys()) - set(filter_prefixes.keys())\n",
    "\n",
    "# Delete files from stage6 with the unique prefixes\n",
    "for prefix in unique_stage6_prefixes:\n",
    "    for filename in stage6_prefixes[prefix]:\n",
    "        file_path = os.path.join(stage6_dir, filename)\n",
    "        os.remove(file_path)\n",
    "\n",
    "# Save the deleted prefixes to a CSV file\n",
    "csv_file = \"deleted_prefixes.csv\"\n",
    "with open(csv_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"prefix\"])\n",
    "    for prefix in unique_stage6_prefixes:\n",
    "        writer.writerow([prefix])\n",
    "\n",
    "# Print the number of unique prefixes that were deleted\n",
    "print(f\"Deleted {len(unique_stage6_prefixes)} unique prefixes. CSV saved to {csv_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92735862",
   "metadata": {},
   "source": [
    "#### vi. Comparacion de prefijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Define directories\n",
    "stage4_dir = \"ANPR2.v1i.yolov8/train_stage5_options/images\"\n",
    "#train_dir = \"ANPR2.v1i.yolov8/train/images\"\n",
    "train_dir = \"ANPR2.v1i.yolov8/train_del/train_filter_v2/images\"\n",
    "\n",
    "def get_unique_prefixes(directory):\n",
    "    prefixes = set()\n",
    "    for filename in os.listdir(directory):\n",
    "        if '.rf' in filename:\n",
    "            prefix = filename.split('.rf')[0]\n",
    "            prefixes.add(prefix)\n",
    "    return prefixes\n",
    "\n",
    "def get_unique_extensions(directory):\n",
    "    extensions = set()\n",
    "    for filename in os.listdir(directory):\n",
    "        ext = os.path.splitext(filename)[1].lower()\n",
    "        extensions.add(ext)\n",
    "    return extensions\n",
    "\n",
    "def get_unique_dimensions(directory):\n",
    "    dimensions = set()\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                dimensions.add(img.size)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return dimensions\n",
    "\n",
    "# Get unique prefixes\n",
    "prefixes_stage4 = get_unique_prefixes(stage4_dir)\n",
    "prefixes_train = get_unique_prefixes(train_dir)\n",
    "missing_prefixes = prefixes_train - prefixes_stage4\n",
    "\n",
    "print(\"Count of unique prefixes in stage images directory:\", len(prefixes_stage4))\n",
    "print(\"Count of unique prefixes in mod images directory:\", len(prefixes_train))\n",
    "print(\"\\nPrefixes missing in stage4 images directory:\")\n",
    "for prefix in sorted(missing_prefixes):\n",
    "    print(prefix)\n",
    "\n",
    "# Get unique extensions and dimensions for stage4 directory\n",
    "extensions_stage4 = get_unique_extensions(stage4_dir)\n",
    "dimensions_stage4 = get_unique_dimensions(stage4_dir)\n",
    "\n",
    "print(\"\\nUnique extensions in stage4 images directory:\")\n",
    "for ext in sorted(extensions_stage4):\n",
    "    print(ext)\n",
    "print(\"\\nUnique dimensions in stage4 images directory:\")\n",
    "for dim in sorted(dimensions_stage4):\n",
    "    print(dim)\n",
    "\n",
    "# Get unique extensions and dimensions for train directory\n",
    "extensions_train = get_unique_extensions(train_dir)\n",
    "dimensions_train = get_unique_dimensions(train_dir)\n",
    "\n",
    "print(\"\\nUnique extensions in train images directory:\")\n",
    "for ext in sorted(extensions_train):\n",
    "    print(ext)\n",
    "print(\"\\nUnique dimensions in train images directory:\")\n",
    "for dim in sorted(dimensions_train):\n",
    "    print(dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d8857",
   "metadata": {},
   "source": [
    "### 2.2 Dataset \"Peru Plate Numbers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7692220",
   "metadata": {},
   "source": [
    "#### i. Primer filtro de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12167409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define paths\n",
    "source_dir = \"Peru Plate Numbers.v3i.yolov8/train/images\"\n",
    "target_dir = \"Peru Plate Numbers.v3i.yolov8/train_filter_v2/images\"\n",
    "\n",
    "# Ensure target directories exist\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Get list of images\n",
    "image_files = sorted(os.listdir(source_dir))\n",
    "\n",
    "# Group images by their prefix before '.rf.'\n",
    "grouped_images = defaultdict(list)\n",
    "\n",
    "for image in image_files:\n",
    "    if \".rf.\" in image:\n",
    "        prefix = image.split(\".rf.\")[0]  # Extract prefix before .rf.\n",
    "        grouped_images[prefix].append(image)\n",
    "\n",
    "# Identify the original images and copy them\n",
    "for prefix, group in grouped_images.items():\n",
    "    group.sort()  # Sort within the group (to maintain order)\n",
    "    \n",
    "    # Assume the last image in sorted order is the original\n",
    "    original_image = group[-1]  \n",
    "\n",
    "    # Copy the original image to the target directory\n",
    "    src_path = os.path.join(source_dir, original_image)\n",
    "    dest_path = os.path.join(target_dir, original_image)\n",
    "    shutil.copy2(src_path, dest_path)\n",
    "\n",
    "print(\"Original images copied successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a64d5",
   "metadata": {},
   "source": [
    "#### ii. Imprimir numero de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c53bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define paths\n",
    "source_dir = \"Peru Plate Numbers.v3i.yolov8/train/images\"\n",
    "target_dir = \"Peru Plate Numbers.v3i.yolov8/train_filter_v2/images\"\n",
    "\n",
    "# Function to count image files in a directory\n",
    "def count_images(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        return 0  # Return 0 if the directory does not exist\n",
    "    return len([f for f in os.listdir(directory) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "\n",
    "# Count images in both directories\n",
    "source_count = count_images(source_dir)\n",
    "target_count = count_images(target_dir)\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of images in source directory ({source_dir}): {source_count}\")\n",
    "print(f\"Number of images in target directory ({target_dir}): {target_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d036f5",
   "metadata": {},
   "source": [
    "#### iii. Stage 3: Depuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "# Directories and CSV files\n",
    "train_dir = \"Peru Plate Numbers.v3i.yolov8/train/images\"\n",
    "filter_v2_dir = \"Peru Plate Numbers.v3i.yolov8/train_filter_v2/images\"\n",
    "filter_v3_dir = \"Peru Plate Numbers.v3i.yolov8/train_filter_v3/images\"\n",
    "csv_input = \"limpieza_dataset_placas_stage-dataset2.csv\"\n",
    "csv_output = \"dataset2_stage2_remaining.csv\"\n",
    "\n",
    "def get_prefix(filename):\n",
    "    # Extract the part before \".rf\" (keeps the _jpg if present)\n",
    "    return filename.split(\".rf\")[0]\n",
    "\n",
    "def build_prefix_mapping(directory):\n",
    "    mapping = {}\n",
    "    for f in os.listdir(directory):\n",
    "        if \".rf\" in f:\n",
    "            prefix = get_prefix(f)\n",
    "            mapping.setdefault(prefix, []).append(f)\n",
    "    return mapping\n",
    "\n",
    "# Build mappings for train and filter_v2 directories.\n",
    "train_mapping = build_prefix_mapping(train_dir)\n",
    "filter_v2_mapping = build_prefix_mapping(filter_v2_dir)\n",
    "\n",
    "# Process CSV file and build set of prefixes to review.\n",
    "review_prefixes = set()\n",
    "with open(csv_input, newline=\"\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        if row:\n",
    "            entry = row[0].strip()\n",
    "            prefix_csv = entry.split(\".rf\")[0]\n",
    "            review_prefixes.add(prefix_csv)\n",
    "\n",
    "# Save the review prefixes to a CSV file.\n",
    "with open(csv_output, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"prefix\"])\n",
    "    for prefix in review_prefixes:\n",
    "        writer.writerow([prefix])\n",
    "print(f\"Saved {len(review_prefixes)} prefixes to {csv_output}\")\n",
    "\n",
    "# Ensure destination folder exists.\n",
    "if not os.path.exists(filter_v3_dir):\n",
    "    os.makedirs(filter_v3_dir)\n",
    "\n",
    "# For each prefix needing review, select one remaining image from train (excluding the filter_v2 choice)\n",
    "selected_count = 0\n",
    "for prefix in review_prefixes:\n",
    "    train_images = train_mapping.get(prefix, [])\n",
    "    if not train_images:\n",
    "        print(f\"No train images found for prefix: {prefix}\")\n",
    "        continue\n",
    "    # In filter_v2 there should be one image per prefix.\n",
    "    v2_images = filter_v2_mapping.get(prefix, [])\n",
    "    chosen_v2 = v2_images[0] if v2_images else None\n",
    "    # Exclude the filter_v2 image from train images.\n",
    "    remaining = [img for img in train_images if img != chosen_v2]\n",
    "    if not remaining:\n",
    "        print(f\"No remaining images for prefix: {prefix}\")\n",
    "        continue\n",
    "    remaining.sort()\n",
    "    selected_img = remaining[0]\n",
    "    src_path = os.path.join(train_dir, selected_img)\n",
    "    dst_path = os.path.join(filter_v3_dir, selected_img)\n",
    "    shutil.copy(src_path, dst_path)\n",
    "    selected_count += 1\n",
    "\n",
    "print(f\"Copied {selected_count} images to {filter_v3_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19e578",
   "metadata": {},
   "source": [
    "#### iv. Stage 4: Depuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e063bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "# Define directories and CSV file\n",
    "train_dir = \"Peru Plate Numbers.v3i.yolov8/train/images\"\n",
    "filter_v2_dir = \"Peru Plate Numbers.v3i.yolov8/train_filter_v2/images\"\n",
    "filter_v3_dir = \"Peru Plate Numbers.v3i.yolov8/train_filter_v3/images\"\n",
    "filter_v4_dir = \"Peru Plate Numbers.v3i.yolov8/train_filter_v4/images\"\n",
    "csv_d2stage3 = \"limpieza_dataset_placas_stage-d2stage3.csv\"\n",
    "\n",
    "def get_prefix(filename):\n",
    "    # Returns the part before \".rf\"\n",
    "    return filename.split(\".rf\")[0]\n",
    "\n",
    "def build_prefix_mapping(directory):\n",
    "    \"\"\"\n",
    "    Builds a dictionary mapping each prefix (extracted from filenames) to a list of filenames.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for f in os.listdir(directory):\n",
    "        if \".rf\" in f:\n",
    "            prefix = get_prefix(f)\n",
    "            mapping.setdefault(prefix, []).append(f)\n",
    "    return mapping\n",
    "\n",
    "# 1. Build mapping for train images (prefix -> list of filenames)\n",
    "train_mapping = build_prefix_mapping(train_dir)\n",
    "\n",
    "# Build mapping for filter_v2 images (one image per prefix)\n",
    "v2_mapping = {}\n",
    "for f in os.listdir(filter_v2_dir):\n",
    "    if \".rf\" in f:\n",
    "        prefix = get_prefix(f)\n",
    "        v2_mapping[prefix] = f\n",
    "\n",
    "# Build mapping for filter_v3 images (one image per prefix)\n",
    "v3_mapping = {}\n",
    "for f in os.listdir(filter_v3_dir):\n",
    "    if \".rf\" in f:\n",
    "        prefix = get_prefix(f)\n",
    "        v3_mapping[prefix] = f\n",
    "\n",
    "# 2. Read CSV file and extract unique prefixes to review\n",
    "review_prefixes = set()\n",
    "with open(csv_d2stage3, newline=\"\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        if row:\n",
    "            entry = row[0].strip()\n",
    "            prefix = entry.split(\".rf\")[0]\n",
    "            review_prefixes.add(prefix)\n",
    "print(f\"Found {len(review_prefixes)} prefixes to review from CSV.\")\n",
    "\n",
    "# 3. For each prefix, determine the remaining candidate image and copy it to filter_v4\n",
    "if not os.path.exists(filter_v4_dir):\n",
    "    os.makedirs(filter_v4_dir)\n",
    "\n",
    "copied_count = 0\n",
    "for prefix in review_prefixes:\n",
    "    # Get the list of train images for this prefix\n",
    "    imgs = train_mapping.get(prefix, [])\n",
    "    if not imgs:\n",
    "        print(f\"No train images found for prefix: {prefix}\")\n",
    "        continue\n",
    "    # Exclude the image from filter_v2 (the one randomly picked)\n",
    "    if prefix in v2_mapping:\n",
    "        imgs = [img for img in imgs if img != v2_mapping[prefix]]\n",
    "    # Exclude the image from filter_v3 (the non-original candidate)\n",
    "    if prefix in v3_mapping:\n",
    "        imgs = [img for img in imgs if img != v3_mapping[prefix]]\n",
    "    # The remaining image should be the original candidate\n",
    "    if len(imgs) == 1:\n",
    "        src = os.path.join(train_dir, imgs[0])\n",
    "        dst = os.path.join(filter_v4_dir, imgs[0])\n",
    "        shutil.copy(src, dst)\n",
    "        copied_count += 1\n",
    "    else:\n",
    "        print(f\"Unexpected number of remaining images for prefix '{prefix}': {imgs}\")\n",
    "\n",
    "print(f\"Copied {copied_count} images to {filter_v4_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1309eb3d",
   "metadata": {},
   "source": [
    "#### v. Stage 5. Unir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "# Define directories\n",
    "filter_v2_dir = \"Peru Plate Numbers.v3i.yolov8/train_filter_v2/images\"\n",
    "filter_v3_dir = \"Peru Plate Numbers.v3i.yolov8/train_filter_v3/images\"\n",
    "filter_v4_dir = \"Peru Plate Numbers.v3i.yolov8/train_filter_v4/images\"\n",
    "filter_v5_dir = \"Peru Plate Numbers.v3i.yolov8/train_filter_v5/images\"\n",
    "\n",
    "# CSV files\n",
    "csv_v2 = \"limpieza_dataset_placas_stage-dataset2.csv\"\n",
    "csv_v3 = \"limpieza_dataset_placas_stage-d2stage3.csv\"\n",
    "\n",
    "def load_csv_set(csv_file):\n",
    "    \"\"\"Reads the CSV file and returns a set of image identifiers (filenames without extension).\"\"\"\n",
    "    s = set()\n",
    "    with open(csv_file, newline=\"\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                # Each row is assumed to have one entry, e.g.\n",
    "                # 20231009_192443_jpg.rf.da5d595c40e613d9878567c3c34f2c1d\n",
    "                s.add(row[0].strip())\n",
    "    return s\n",
    "\n",
    "# Load CSV identifiers\n",
    "csv_v2_set = load_csv_set(csv_v2)\n",
    "csv_v3_set = load_csv_set(csv_v3)\n",
    "\n",
    "# Create destination folder if it doesn't exist\n",
    "os.makedirs(filter_v5_dir, exist_ok=True)\n",
    "\n",
    "copied_count = 0\n",
    "\n",
    "# Subset 1: Images from filter_v2 not in csv_v2_set\n",
    "for filename in os.listdir(filter_v2_dir):\n",
    "    if filename.lower().endswith(\".jpg\"):\n",
    "        identifier = os.path.splitext(filename)[0]\n",
    "        if identifier not in csv_v2_set:\n",
    "            src_path = os.path.join(filter_v2_dir, filename)\n",
    "            dst_path = os.path.join(filter_v5_dir, filename)\n",
    "            shutil.copy(src_path, dst_path)\n",
    "            copied_count += 1\n",
    "\n",
    "# Subset 2: Images from filter_v3 not in csv_v3_set\n",
    "for filename in os.listdir(filter_v3_dir):\n",
    "    if filename.lower().endswith(\".jpg\"):\n",
    "        identifier = os.path.splitext(filename)[0]\n",
    "        if identifier not in csv_v3_set:\n",
    "            src_path = os.path.join(filter_v3_dir, filename)\n",
    "            dst_path = os.path.join(filter_v5_dir, filename)\n",
    "            shutil.copy(src_path, dst_path)\n",
    "            copied_count += 1\n",
    "\n",
    "# Subset 3: All images from filter_v4\n",
    "for filename in os.listdir(filter_v4_dir):\n",
    "    if filename.lower().endswith(\".jpg\"):\n",
    "        src_path = os.path.join(filter_v4_dir, filename)\n",
    "        dst_path = os.path.join(filter_v5_dir, filename)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        copied_count += 1\n",
    "\n",
    "print(f\"Copied {copied_count} images to {filter_v5_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b27a14",
   "metadata": {},
   "source": [
    "#### vi. Eliminar imagenes de motocicletas y placas obstruidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Define directories and CSV output file\n",
    "filter_v6_dir = r\"Peru Plate Numbers.v3i.yolov8\\train_filter_v6\\images\"\n",
    "del_dir = r\"Peru Plate Numbers.v3i.yolov8\\train_del\\images\"\n",
    "csv_output = \"deleted_prefixes_dataset2.csv\"\n",
    "\n",
    "def get_prefix(filename):\n",
    "    # Return the part before \".rf\"\n",
    "    return filename.split(\".rf\")[0]\n",
    "\n",
    "def collect_prefixes(directory):\n",
    "    prefixes = set()\n",
    "    for f in os.listdir(directory):\n",
    "        if \".rf\" in f:\n",
    "            prefixes.add(get_prefix(f))\n",
    "    return prefixes\n",
    "\n",
    "# Get prefixes from both directories\n",
    "v6_prefixes = collect_prefixes(filter_v6_dir)\n",
    "del_prefixes = collect_prefixes(del_dir)\n",
    "\n",
    "# Determine prefixes in v6 that are not in del\n",
    "prefixes_to_delete = v6_prefixes - del_prefixes\n",
    "\n",
    "# Save these prefixes to CSV\n",
    "with open(csv_output, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"prefix\"])\n",
    "    for prefix in sorted(prefixes_to_delete):\n",
    "        writer.writerow([prefix])\n",
    "print(f\"Saved {len(prefixes_to_delete)} prefixes to {csv_output}\")\n",
    "\n",
    "# Delete images in filter_v6 with the identified prefixes\n",
    "deleted_count = 0\n",
    "for filename in os.listdir(filter_v6_dir):\n",
    "    if \".rf\" in filename:\n",
    "        prefix = get_prefix(filename)\n",
    "        if prefix in prefixes_to_delete:\n",
    "            os.remove(os.path.join(filter_v6_dir, filename))\n",
    "            deleted_count += 1\n",
    "\n",
    "print(f\"Deleted {deleted_count} images from {filter_v6_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
